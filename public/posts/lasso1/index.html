<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Beyond LASSO | Random Learnings</title>
<meta name="keywords" content="shrinkage, lasso">
<meta name="description" content="A brief exploration of LASSO and its variants">
<meta name="author" content="">
<link rel="canonical" href="https://VLDrenth.github.io/posts/lasso1/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.d72444526d7ecbdb0015438a7fa89054a658bf759d0542e2e5df81ce94b493ee.css" integrity="sha256-1yREUm1&#43;y9sAFUOKf6iQVKZYv3WdBULi5d&#43;BzpS0k&#43;4=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://VLDrenth.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://VLDrenth.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://VLDrenth.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://VLDrenth.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://VLDrenth.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://VLDrenth.github.io/posts/lasso1/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css" integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js" integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
    onload="renderMathInElement(document.body, {
        delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '$', right: '$', display: false}
        ],
        throwOnError: false
    });"></script>
<meta property="og:url" content="https://VLDrenth.github.io/posts/lasso1/">
  <meta property="og:site_name" content="Random Learnings">
  <meta property="og:title" content="Beyond LASSO">
  <meta property="og:description" content="A brief exploration of LASSO and its variants">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2024-12-15T00:00:00+00:00">
    <meta property="article:modified_time" content="2024-12-15T00:00:00+00:00">
    <meta property="article:tag" content="Shrinkage">
    <meta property="article:tag" content="Lasso">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Beyond LASSO">
<meta name="twitter:description" content="A brief exploration of LASSO and its variants">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://VLDrenth.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Beyond LASSO",
      "item": "https://VLDrenth.github.io/posts/lasso1/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Beyond LASSO",
  "name": "Beyond LASSO",
  "description": "A brief exploration of LASSO and its variants",
  "keywords": [
    "shrinkage", "lasso"
  ],
  "articleBody": "The basics One of the first ‘machine learning’ many will come across is LASSO regression. This may suggest that it’s an elementary method; something to get you started on machine learning before moving to the more interesting topics. But there is a sea of methods that are adaptions of LASSO. The goal of this blog post is to give a brief overview of the landscape of these methods and where they might be useful. Understanding these other methods, many of which generalize the LASSO, should also help us gain a better understanding (and appreciation) of the ‘simple’ version.\n$$\ry = X\\beta + \\varepsilon.\r$$If the number of features is not too high, the OLS estimator gives good estimates of the coefficients and all is well. However, when the number of features increases, this increases the total variance of the OLS estimates and may lead to worse predictions out of sample. Estimates for the effect of a feature are more likely to be very large, which doesn’t tend to generalize well. If we have a lot of features, possibly more than the number of observations, the OLS estimator is not even defined as there is no unique solution to the least-squares problem. One way to circumvent this problem is to simply throw away some of your features. This can be done using domain knowledge, some data analysis, or some automated method. This may feel rather ad-hoc however. An alternative, more principled way, is to use regularization. The key idea is that large coefficient estimates may overestimate the true effect size, so we should simply penalize the size of the coefficient! This simple but powerful idea is the driving idea behind the large literature on shrinkage methods.\nRidge Regression The simplest way to regularize the coefficient size is to add a penalty on the size of the coefficients. A natural way to capture the size of $\\beta$ is to use the Euclidean norm:\n$$\r\\min_{\\boldsymbol{\\beta}} \\left\\{ \\|\\boldsymbol{y} - \\boldsymbol{X\\beta}\\|_2^2 + \\lambda\\|\\boldsymbol{\\beta}\\|_2^2 \\right\\}\r$$This is called Ridge regression, which has a unique minimizer for any positive penalty strength $\\lambda$, even when we have many more features than observations! We can even get the solution in closed form by making use of the fact that the Euclidean norm is differentiable. This is a natural and convenient choice, but the Euclidean norm is only one out of many. Why not use the $L_1$ norm? This is also a viable choice, and is what gives us the LASSO estimator.\nLASSO Regression $$\r\\min_{\\boldsymbol{\\beta}} \\left\\{\\|\\boldsymbol{y} - \\boldsymbol{X\\beta}\\|_2^2 + \\lambda\\sum_{j=1}^p|\\boldsymbol{\\beta}_j| \\right\\}.\r$$While similar to Ridge, there are many key differences to explore. First a practical issue: the $L_1$ norm is not differentiable, so there is no closed form solution anymore. Thankfully, it is true that all valid norms are convex (how lucky!), which means this is still an ’easy’ problem to solve. More on that later.\nMore interesting, is how the two penalties differ in the resulting estimates. To explain this, I can’t do any better than the bible of classical ML: the Elements of Statistical Learning. The gist of the argument is this. We can consider the minimization problems above as Lagrangians of a constrained optimization problem. Both Ridge and LASSO aim to minimize the sum of squared errors, but differ in their constraint. The Ridge constraint is $\\| \\boldsymbol{\\beta} \\|_2^2 \\leq t$ and analously the LASSO constraint is $\\| \\boldsymbol{\\beta} \\|_1 \\leq t$ (Exercise: What is the value of $t$?). Below you see the feasible region in a simple 2D case as well as the contours which represent a value of the squared loss for some set of coefficients. Notice that the LASSO region has a sharp edge and may intersect with a contour line at a corner, where one of the coefficients equals zero. Ridge, on the other hand, is smooth and will always have non-zero coefficients. This is the crucial difference between Ridge and LASSO: LASSO sets some subset of the predictor’s coefficients to zero and thus searches for a sparse solution.\nThe math behind LASSO While this seems intuitive, let’s make this a bit more precise. Recall that the LASSO objective is a convex optimization problem but not differentiable. While a differentiable (and convex) objective may be solved by setting the gradient to zero, here we need to rely on the subgradient, a generalized notion of the derivative. This gives us the condition for optimal $\\beta$ as:\n$$\rX^T(Y - X\\hat{\\beta}) = \\lambda s\r$$$$\rs_j = \\begin{cases} -1, \u0026 \\text{if } \\beta_j \u003c 0 \\\\\r1 \u0026 \\text{if } \\beta_j \u003e 0 \\\\\r[-1, 1] \u0026 \\text{if } \\beta_j = 0\r\\end{cases}.\r$$Estimating LASSO With this in mind we can look at an algorithm to solve for $\\hat{\\beta}$. Suppose we only have 1 regressor, and that we are minimizing $\\frac{1}{2N}\\sum_{i=1}^N(y_i - x_i\\beta)^2 + \\lambda|\\beta|$. By taking the (sub)-derivative and setting this equal to zero we obtain the following solution which depends on the penalization strength:\n$$\r\\hat{\\beta} =\r\\begin{cases} \\displaystyle\\frac{1}{N}\\langle x, y \\rangle - \\lambda, \u0026 \\text{if } \\frac{1}{N}\\langle x, y \\rangle \u003e \\lambda \\\\ 0, \u0026 \\text{if } \\frac{1}{N} |\\langle z, y \\rangle| \\leq \\lambda \\\\ \\displaystyle\\frac{1}{N}\\langle x, y \\rangle + \\lambda, \u0026 \\text{if } \\frac{1}{N}\\langle x, y \\rangle \u003c -\\lambda\r\\end{cases}\r$$$$ \\beta = \\mathcal{S}_\\lambda(\\frac{1}{N}\\langle x, y \\rangle) $$ where $ \\mathcal{S}_\\lambda(u) = sign(u)max(0, |u| - \\lambda) $. First off, note that when $\\lambda$ is set to zero we indeed recover the OLS estimator. But when it’s not equal to zero, the LASSO sets the coefficient to zero when the covariance is too small. Even when the covariance is larger than the $\\lambda$, it is shrunk towards zero. This clearly showcases how LASSO shrinks estimates towards zero and may set them to exactly zero.\nThis is for a single covariate, which is not really what we’re here for, so let’s extend this to many regressors. We can use cyclical coordinate descent, which iterates over all coefficients $1, \\dots, p$. The algorithm then minimizes with respect to a single $\\beta_j$ keeping all others fixed. Doing this while iterating over coefficients until convergence yields an esimtate of the multivariate LASSO. It turns out that by rewriting the problem a little bit, we can use the univariate solution to do this! The objective can be written as:\n$$\r\\frac{1}{2N} \\sum_{i=1}^N \\left( y_i - \\sum_{k \\neq j} x_{ik} \\beta_k - x_{ij} \\beta_j \\right)^2 +\r\\lambda \\sum_{k \\neq j} |\\beta_k| + \\lambda |\\beta_j|\r$$If we are only optimizing for $\\beta_j$, this is a univariate regression of the ‘partial residuals’ $r_j = y - X_{-j}\\beta_{-j}$ on $x_j$, which has an explicit solution. Updating $\\hat{\\beta}_j \\leftarrow \\mathcal{S}_\\lambda(\\frac{1}{N}\\langle x_j, r_j \\rangle)$. It is interesting to note what happens when our predictors are all orthogonal. In this case we have $\\langle x_j, r_j \\rangle = \\langle x_j, y \\rangle$. In other words, the estimate for each predictors is just the univariate solution, meaning the whole problem can be solved in closed form! So if you are concerned about convergence and speed, you can always orthogonalize your features and get the solution explicitly.\nThe convergence properties of this algorithm and generally the class of coordinate descent methods are interesting and well-studied, but I’ll just say this convergences under normal circumstances and leave the details for another day. In practice more sophisticated methods are used, that construct the optimal estimates not just for a single value of $\\lambda$ but for a full sequence or path.\nOther Sparse Penalties Most (if not all) of the above may already be familiar to you from an intro to machine learning class. The remainder of this post hopefully shows at least 1 new method that might be useful for whatever problem you’re working on. I’ll cover the following versions or generalizations of the LASSO:\nElastic Net Adaptive LASSO Relaxed LASSO Group LASSO ",
  "wordCount" : "1298",
  "inLanguage": "en",
  "datePublished": "2024-12-15T00:00:00Z",
  "dateModified": "2024-12-15T00:00:00Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://VLDrenth.github.io/posts/lasso1/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Random Learnings",
    "logo": {
      "@type": "ImageObject",
      "url": "https://VLDrenth.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://VLDrenth.github.io/" accesskey="h" title="Random Learnings (Alt + H)">Random Learnings</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://VLDrenth.github.io/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="https://VLDrenth.github.io/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="https://VLDrenth.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Beyond LASSO
    </h1>
    <div class="post-description">
      A brief exploration of LASSO and its variants
    </div>
    <div class="post-meta"><span title='2024-12-15 00:00:00 +0000 UTC'>December 15, 2024</span>&nbsp;·&nbsp;7 min

</div>
  </header> 
  <div class="post-content"><h1 id="the-basics">The basics<a hidden class="anchor" aria-hidden="true" href="#the-basics">#</a></h1>
<p>One of the first &lsquo;machine learning&rsquo; many will come across is LASSO regression. This may suggest that it&rsquo;s an elementary method; something to get you started on machine learning before moving to the more interesting topics. But there is a sea of methods that are adaptions of LASSO. The goal of this blog post is to give a brief overview of the landscape of these methods and where they might be useful. Understanding these other methods, many of which generalize the LASSO, should also help us gain a better understanding (and appreciation) of the &lsquo;simple&rsquo; version.</p>
$$
y = X\beta + \varepsilon.
$$<p>If the number of features is not too high, the OLS estimator gives good estimates of the coefficients and all is well. However, when the number of features increases, this increases the total variance of the OLS estimates and may lead to worse predictions out of sample. Estimates for the effect of a feature are more likely to be very large, which doesn&rsquo;t tend to generalize well. If we have a lot of features, possibly more than the number of observations, the OLS estimator is not even defined as there is no unique solution to the least-squares problem. One way to circumvent this problem is to simply throw away some of your features. This can be done using domain knowledge, some data analysis, or some automated method. This may feel rather ad-hoc however. An alternative, more principled way, is to use <em>regularization</em>. The key idea is that large coefficient estimates may overestimate the <em>true</em> effect size, so we should simply penalize the size of the coefficient! This simple but powerful idea is the driving idea behind the large literature on shrinkage methods.</p>
<h1 id="ridge-regression">Ridge Regression<a hidden class="anchor" aria-hidden="true" href="#ridge-regression">#</a></h1>
<p>The simplest way to regularize the coefficient size is to add a penalty on the size of the coefficients. A natural way to capture the size of $\beta$ is to use the Euclidean norm:</p>
$$
\min_{\boldsymbol{\beta}} \left\{ \|\boldsymbol{y} - \boldsymbol{X\beta}\|_2^2 + \lambda\|\boldsymbol{\beta}\|_2^2 \right\}
$$<p>This is called Ridge regression, which has a unique minimizer for any positive penalty strength $\lambda$, even when we have many more features than observations! We can even get the solution in closed form by making use of the fact that the Euclidean norm is differentiable. This is a natural and convenient choice, but the Euclidean norm is only one out of many. Why not use the $L_1$ norm? This is also a viable choice, and is what gives us the LASSO estimator.</p>
<h1 id="lasso-regression">LASSO Regression<a hidden class="anchor" aria-hidden="true" href="#lasso-regression">#</a></h1>
$$
\min_{\boldsymbol{\beta}} \left\{\|\boldsymbol{y} - \boldsymbol{X\beta}\|_2^2 + \lambda\sum_{j=1}^p|\boldsymbol{\beta}_j| \right\}.
$$<p>While similar to Ridge, there are many key differences to explore. First a practical issue: the $L_1$ norm is not differentiable, so there is no closed form solution anymore. Thankfully, it is true that all valid norms are convex (how lucky!), which means this is still an &rsquo;easy&rsquo; problem to solve. More on that later.</p>
<p>More interesting, is how the two penalties differ in the resulting estimates. To explain this, I can&rsquo;t do any better than the bible of classical ML: the Elements of Statistical Learning. The gist of the argument is this. We can consider the minimization problems above as Lagrangians of a constrained optimization problem. Both Ridge and LASSO aim to minimize the sum of squared errors, but differ in their constraint. The Ridge constraint is $\| \boldsymbol{\beta} \|_2^2 \leq t$ and analously the LASSO constraint is $\| \boldsymbol{\beta} \|_1 \leq t$ (Exercise: What is the value of $t$?). Below you see the feasible region in a simple 2D case as well as the contours which represent a value of the squared loss for some set of coefficients. Notice that the LASSO region has a sharp edge and may intersect with a contour line at a corner, where one of the coefficients equals zero. Ridge, on the other hand, is smooth and will always have non-zero coefficients. This is the crucial difference between Ridge and LASSO: LASSO sets some subset of the predictor&rsquo;s coefficients to zero and thus searches for a <em>sparse</em> solution.</p>
<h2 id="the-math-behind-lasso">The math behind LASSO<a hidden class="anchor" aria-hidden="true" href="#the-math-behind-lasso">#</a></h2>
<p>While this seems intuitive, let&rsquo;s make this a bit more precise. Recall that the LASSO objective is a convex optimization problem but not differentiable. While a differentiable (and convex) objective may be solved by setting the gradient to zero, here we need to rely on the <em>subgradient</em>, a generalized notion of the derivative. This gives us the condition for optimal $\beta$ as:</p>
$$
X^T(Y - X\hat{\beta}) = \lambda s
$$$$
s_j = \begin{cases} 
-1, & \text{if } \beta_j < 0 \\
1 & \text{if } \beta_j > 0 \\
[-1, 1] & \text{if } \beta_j = 0
\end{cases}.
$$<h2 id="estimating-lasso">Estimating LASSO<a hidden class="anchor" aria-hidden="true" href="#estimating-lasso">#</a></h2>
<p>With this in mind we can look at an algorithm to solve for $\hat{\beta}$. Suppose we only have 1 regressor, and that we are minimizing $\frac{1}{2N}\sum_{i=1}^N(y_i - x_i\beta)^2 + \lambda|\beta|$. By taking the (sub)-derivative and setting this equal to zero we obtain the following solution which depends on the penalization strength:</p>
$$
\hat{\beta} =
\begin{cases} 
    \displaystyle\frac{1}{N}\langle x, y \rangle - \lambda, & \text{if } \frac{1}{N}\langle x, y \rangle > \lambda \\ 
    0, & \text{if } \frac{1}{N} |\langle z, y \rangle| \leq \lambda \\ 
    \displaystyle\frac{1}{N}\langle x, y \rangle + \lambda, & \text{if } \frac{1}{N}\langle x, y \rangle < -\lambda
\end{cases}
$$$$ \beta = \mathcal{S}_\lambda(\frac{1}{N}\langle x, y \rangle) $$<p> where $ \mathcal{S}_\lambda(u) = sign(u)max(0, |u| - \lambda) $. First off, note that when $\lambda$ is set to zero we indeed recover the OLS estimator. But when it&rsquo;s not equal to zero, the LASSO sets the coefficient to zero when the covariance is too small. Even when the covariance is larger than the $\lambda$, it is shrunk towards zero. This clearly showcases how LASSO shrinks estimates towards zero and may set them to <em>exactly</em> zero.</p>
<p>This is for a single covariate, which is not really what we&rsquo;re here for, so let&rsquo;s extend this to many regressors. We can use <em>cyclical coordinate descent</em>, which iterates over all coefficients $1, \dots, p$. The algorithm then minimizes with respect to a single $\beta_j$ keeping all others fixed. Doing this while iterating over coefficients until convergence yields an esimtate of the multivariate LASSO. It turns out that by rewriting the problem a little bit, we can use the univariate solution to do this! The objective can be written as:</p>
$$
\frac{1}{2N} \sum_{i=1}^N \left( y_i - \sum_{k \neq j} x_{ik} \beta_k - x_{ij} \beta_j \right)^2 +
\lambda \sum_{k \neq j} |\beta_k| + \lambda |\beta_j|
$$<p>If we are only optimizing for $\beta_j$, this is a univariate regression of the &lsquo;partial residuals&rsquo; $r_j = y - X_{-j}\beta_{-j}$ on $x_j$, which has an explicit solution. Updating $\hat{\beta}_j \leftarrow \mathcal{S}_\lambda(\frac{1}{N}\langle x_j, r_j \rangle)$. It is interesting to note what happens when our predictors are all orthogonal. In this case we have $\langle x_j, r_j \rangle = \langle x_j, y \rangle$.
In other words, the estimate for each predictors is just the univariate solution, meaning the whole problem can be solved in closed form! So if you are concerned about convergence and speed, you can always orthogonalize your features and get the solution explicitly.</p>
<p>The convergence properties of this algorithm and generally the class of coordinate descent methods are interesting and well-studied, but I&rsquo;ll just say this convergences under normal circumstances and leave the details for another day. In practice more sophisticated methods are used, that construct the optimal estimates not just for a single value of $\lambda$ but for a full sequence or <em>path</em>.</p>
<h2 id="other-sparse-penalties">Other Sparse Penalties<a hidden class="anchor" aria-hidden="true" href="#other-sparse-penalties">#</a></h2>
<p>Most (if not all) of the above may already be familiar to you from an intro to machine learning class. The remainder of this post hopefully shows at least 1 new method that might be useful for whatever problem you&rsquo;re working on. I&rsquo;ll cover the following versions or generalizations of the LASSO:</p>
<ul>
<li>Elastic Net</li>
<li>Adaptive LASSO</li>
<li>Relaxed LASSO</li>
<li>Group LASSO</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://VLDrenth.github.io/tags/shrinkage/">Shrinkage</a></li>
      <li><a href="https://VLDrenth.github.io/tags/lasso/">Lasso</a></li>
    </ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>
        <a href="https://github.com/VLDrenth" rel="noopener noreferrer" target="_blank">GitHub</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }
</script>
</body>

</html>
